{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data from the Internet\n",
    "## Importing flat files from the web: your turn!\n",
    "You are about to import your first file from the web! The flat file you will import will be `'winequality-red.csv'` from the University of California, Irvine's [Machine Learning repository](http://archive.ics.uci.edu/ml/index.html). The flat file contains tabular data of physiochemical properties of red wine, such as pH, alcohol content and citric acid content, along with wine quality rating.\n",
    "\n",
    "The URL of the file is\n",
    "```\n",
    "'https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "```\n",
    "After you import it, you'll check your working directory to confirm that it is there and then you'll load it into a `pandas` DataFrame.\n",
    "\n",
    "**Instructions**\n",
    "* Import the function `urlretrieve` from the subpackage `urllib.request`.\n",
    "* Assign the URL of the file to the variable `url`.\n",
    "* Use the function `urlretrieve()` to save the file locally as `'winequality-red.csv'`.\n",
    "* Execute the remaining code to load `'winequality-red.csv'` in a pandas DataFrame and to print its head to the shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0            7.4              0.70         0.00             1.9      0.076   \n",
      "1            7.8              0.88         0.00             2.6      0.098   \n",
      "2            7.8              0.76         0.04             2.3      0.092   \n",
      "3           11.2              0.28         0.56             1.9      0.075   \n",
      "4            7.4              0.70         0.00             1.9      0.076   \n",
      "\n",
      "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
      "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
      "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
      "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "\n",
      "   alcohol  quality  \n",
      "0      9.4        5  \n",
      "1      9.8        5  \n",
      "2      9.8        5  \n",
      "3      9.8        6  \n",
      "4      9.4        5  \n"
     ]
    }
   ],
   "source": [
    "# Import package\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Assign url of file: url\n",
    "url = 'https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "\n",
    "# Save file locally\n",
    "urlretrieve(url, \"winequality-red.csv\")\n",
    "\n",
    "# Read file into a DataFrame and print its head\n",
    "df = pd.read_csv('winequality-red.csv', sep=';')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opening and reading flat files from the web\n",
    "You have just imported a file from the web, saved it locally and loaded it into a DataFrame. If you just wanted to load a file from the web into a DataFrame without first saving it locally, you can do that easily using `pandas`. In particular, you can use the function `pd.read_csv()` with the URL as the first argument and the separator `sep` as the second argument.\n",
    "\n",
    "The URL of the file, once again, is\n",
    "```\n",
    "'https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "```\n",
    "**Instructions**\n",
    "* Assign the URL of the file to the variable `url`.\n",
    "* Read file into a DataFrame `df` using `pd.read_csv()`, recalling that the separator in the file is `';'`.\n",
    "* Print the head of the DataFrame `df`.\n",
    "* Execute the rest of the code to plot histogram of the first feature in the DataFrame `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0            7.4              0.70         0.00             1.9      0.076   \n",
      "1            7.8              0.88         0.00             2.6      0.098   \n",
      "2            7.8              0.76         0.04             2.3      0.092   \n",
      "3           11.2              0.28         0.56             1.9      0.075   \n",
      "4            7.4              0.70         0.00             1.9      0.076   \n",
      "\n",
      "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
      "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
      "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
      "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "\n",
      "   alcohol  quality  \n",
      "0      9.4        5  \n",
      "1      9.8        5  \n",
      "2      9.8        5  \n",
      "3      9.8        6  \n",
      "4      9.4        5  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\stat_tools\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEcCAYAAADdtCNzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wFmZFdkM/rTAwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23756d1ed68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import packages\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assign url of file: url\n",
    "url = 'https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "\n",
    "# Read file into a DataFrame: df\n",
    "df = pd.read_csv(url, sep = \";\")\n",
    "\n",
    "# Print the head of the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Plot first column of df\n",
    "pd.DataFrame.hist(df.ix[:, 0:1])\n",
    "plt.xlabel('fixed acidity (g(tartaric acid)/dm$^3$)')\n",
    "plt.ylabel('count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing non-flat files from the web\n",
    "Congrats! You've just loaded a flat file from the web into a DataFrame without first saving it locally using the `pandas` function `pd.read_csv()`. This function is super cool because it has close relatives that allow you to load all types of files, not only flat ones. In this interactive exercise, you'll use `pd.read_excel()` to import an Excel spreadsheet.\n",
    "\n",
    "The URL of the spreadsheet is\n",
    "```\n",
    "'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/latitude.xls'\n",
    "```\n",
    "Your job is to use `pd.read_excel()` to read in all of its sheets, print the sheet names and then print the head of the first sheet using its name, not its index.\n",
    "\n",
    "Note that the output of `pd.read_excel()` is a Python dictionary with sheet names as keys and corresponding DataFrames as corresponding values.\n",
    "\n",
    "**Instruction**\n",
    "* Assign the URL of the file to the variable `url`.\n",
    "* Read the file in `url` into a dictionary `xl` using `pd.read_excel()` recalling that, in order to import all sheets you need to pass `None` to the argument `sheetname`.\n",
    "* Print the names of the sheets in the Excel spreadsheet; these will be the keys of the dictionary `xl`.\n",
    "* Print the head of the first sheet using the sheet name, not the index of the sheet! The sheet name is `'1700'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['1700', '1900'])\n",
      "                 country       1700\n",
      "0            Afghanistan  34.565000\n",
      "1  Akrotiri and Dhekelia  34.616667\n",
      "2                Albania  41.312000\n",
      "3                Algeria  36.720000\n",
      "4         American Samoa -14.307000\n"
     ]
    }
   ],
   "source": [
    "# Import package\n",
    "import pandas as pd\n",
    "\n",
    "# Assign url of file: url\n",
    "url = 'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/latitude.xls'\n",
    "\n",
    "# Read in all sheets of Excel file: xl\n",
    "xl = pd.read_excel(url, sheetname = None)\n",
    "\n",
    "# Print the sheetnames to the shell\n",
    "print(xl.keys())\n",
    "\n",
    "# Print the head of the first sheet (using its name, NOT its index)\n",
    "print(pd.read_excel(url, sheetname = \"1700\").head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing HTTP requests in Python using urllib\n",
    "Now that you know the basics behind HTTP GET requests, it's time to perform some of your own. In this interactive exercise, you will ping our very own DataCamp servers to perform a GET request to extract information from our teach page, `\"http://www.datacamp.com/teach/documentation\"`.\n",
    "\n",
    "In the next exercise, you'll extract the HTML itself. Right now, however, you are going to package and send the request and then catch the response.\n",
    "\n",
    "**Instruction**\n",
    "* Import the functions `urlopen` and `Request` from the subpackage `urllib.request`.\n",
    "* Package the request to the url `\"http://www.datacamp.com/teach/documentation\"` using the function `Request()` and assign it to request.\n",
    "* Send the request and catch the response in the variable `response` with the function `urlopen()`.\n",
    "* Run the rest of the code to see the datatype of `response` and to close the connection!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'http.client.HTTPResponse'>\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "# Specify the url\n",
    "url = \"http://www.datacamp.com/teach/documentation\"\n",
    "\n",
    "# This packages the request: request\n",
    "request = Request(url)\n",
    "\n",
    "# Sends the request and catches the response: response\n",
    "response = urlopen(request)\n",
    "\n",
    "# Print the datatype of response\n",
    "print(type(response))\n",
    "\n",
    "# Be polite and close the response!\n",
    "response.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing HTTP request results in Python using urllib\n",
    "You have just just packaged and sent a GET request to `\"http://docs.datacamp.com/teach/\"` and then caught the response. You saw that such a response is a `http.client.HTTPResponse` object. The question remains: what can you do with this response?\n",
    "\n",
    "Well, as it came from an HTML page, you could read it to extract the HTML and, in fact, such a `http.client.HTTPResponse` object has an associated `read()` method. In this exercise, you'll build on your previous great work to extract the response and print the HTML.\n",
    "\n",
    "**Instructions**\n",
    "* Send the request and catch the `response` in the variable response with the function `urlopen()`, as in the previous exercise.\n",
    "* Extract the response using the `read()` method and store the result in the variable `html`.\n",
    "* Print the string `html`.\n",
    "* Hit submit to perform all of the above and to close the response: be tidy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<!DOCTYPE html>\\n<link rel=\"shortcut icon\" href=\"images/favicon.ico\" />\\n<html>\\n\\n  <head>\\n  <meta charset=\"utf-8\">\\n  <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\\n\\n  <title>Home</title>\\n  <meta name=\"description\" content=\"All Documentation on Course Creation\">\\n\\n  <link rel=\"stylesheet\" href=\"/teach/css/main.css\">\\n  <link rel=\"canonical\" href=\"/teach/\">\\n  <link rel=\"alternate\" type=\"application/rss+xml\" title=\"DataCamp Teach Documentation\" href=\"/teach/feed.xml\" />\\n</head>\\n\\n\\n  <body>\\n\\n    <header class=\"site-header\">\\n\\n  <div class=\"wrapper\">\\n\\n    <a class=\"site-title\" href=\"/teach/\">DataCamp Teach Documentation</a>\\n\\n  </div>\\n\\n</header>\\n\\n\\n    <div class=\"page-content\">\\n      <div class=\"wrapper\">\\n        <p>The Teach Documentation has been moved to <a href=\"https://www.datacamp.com/teach/documentation\">https://www.datacamp.com/teach/documentation</a>!</p>\\n\\n<!-- Everybody can teach on DataCamp. The resources on this website explain all the steps to build your own course on DataCamp\\'s interactive data science platform.\\n\\nInterested in partnering with DataCamp? Head over to the [Course Material](/teach/course-material.html) page to get an idea of the requirements to build your own interactive course together with DataCamp!\\n\\n## Table of Contents\\n\\n- [Course Material](/teach/course-material.html) - Content required to build a DataCamp course.\\n- [Video Lectures](/teach/video-lectures.html) - Details on video recording and editing.\\n- [DataCamp Teach](https://www.datacamp.com/teach) - Use the DataCamp Teach website to create DataCamp courses (preferred).\\n- [datacamp R Package](https://github.com/datacamp/datacamp/wiki) - Use R Package to create DataCamp courses (legacy).\\n- [Code DataCamp Exercises](/teach/code-datacamp-exercises.html)\\n- [SCT Design (R)](https://github.com/datacamp/testwhat/wiki)\\n- [SCT Design (Python)](https://github.com/datacamp/pythonwhat/wiki)\\n- [Style Guide](/teach/style-guide.html) -->\\n\\n\\n      </div>\\n    </div>\\n\\n    \\n\\n  </body>\\n\\n</html>\\n'\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "# Specify the url\n",
    "url = \"http://docs.datacamp.com/teach/\"\n",
    "\n",
    "# This packages the request\n",
    "request = Request(url)\n",
    "\n",
    "# Sends the request and catches the response: response\n",
    "response = urlopen(request)\n",
    "\n",
    "# Extract the response: html\n",
    "html = response.read()\n",
    "\n",
    "# Print the html\n",
    "print(html)\n",
    "\n",
    "# Be polite and close the response!\n",
    "response.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing HTTP requests in Python using requests\n",
    "Now that you've got your head and hands around making HTTP requests using the `urllib` package, you're going to figure out how to do the same using the higher-level requests library. You'll once again be pinging DataCamp servers for their `\"http://docs.datacamp.com/teach/\"` page.\n",
    "\n",
    "Note that unlike in the previous exercises using `urllib`, you don't have to close the connection when using requests!\n",
    "\n",
    "**Instructions**\n",
    "* Import the package `requests`.\n",
    "* Assign the URL of interest to the variable `url`.\n",
    "* Package the request to the URL, send the request and catch the response with a single function `requests.get()`, assigning the response to the variable `r`.\n",
    "* Use the text attribute of the object `r` to return the HTML of the webpage as a string; store the result in a variable `text`.\n",
    "* Hit submit to print the HTML of the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<link rel=\"shortcut icon\" href=\"images/favicon.ico\" />\n",
      "<html>\n",
      "\n",
      "  <head>\n",
      "  <meta charset=\"utf-8\">\n",
      "  <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\n",
      "  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
      "\n",
      "  <title>Home</title>\n",
      "  <meta name=\"description\" content=\"All Documentation on Course Creation\">\n",
      "\n",
      "  <link rel=\"stylesheet\" href=\"/teach/css/main.css\">\n",
      "  <link rel=\"canonical\" href=\"/teach/\">\n",
      "  <link rel=\"alternate\" type=\"application/rss+xml\" title=\"DataCamp Teach Documentation\" href=\"/teach/feed.xml\" />\n",
      "</head>\n",
      "\n",
      "\n",
      "  <body>\n",
      "\n",
      "    <header class=\"site-header\">\n",
      "\n",
      "  <div class=\"wrapper\">\n",
      "\n",
      "    <a class=\"site-title\" href=\"/teach/\">DataCamp Teach Documentation</a>\n",
      "\n",
      "  </div>\n",
      "\n",
      "</header>\n",
      "\n",
      "\n",
      "    <div class=\"page-content\">\n",
      "      <div class=\"wrapper\">\n",
      "        <p>The Teach Documentation has been moved to <a href=\"https://www.datacamp.com/teach/documentation\">https://www.datacamp.com/teach/documentation</a>!</p>\n",
      "\n",
      "<!-- Everybody can teach on DataCamp. The resources on this website explain all the steps to build your own course on DataCamp's interactive data science platform.\n",
      "\n",
      "Interested in partnering with DataCamp? Head over to the [Course Material](/teach/course-material.html) page to get an idea of the requirements to build your own interactive course together with DataCamp!\n",
      "\n",
      "## Table of Contents\n",
      "\n",
      "- [Course Material](/teach/course-material.html) - Content required to build a DataCamp course.\n",
      "- [Video Lectures](/teach/video-lectures.html) - Details on video recording and editing.\n",
      "- [DataCamp Teach](https://www.datacamp.com/teach) - Use the DataCamp Teach website to create DataCamp courses (preferred).\n",
      "- [datacamp R Package](https://github.com/datacamp/datacamp/wiki) - Use R Package to create DataCamp courses (legacy).\n",
      "- [Code DataCamp Exercises](/teach/code-datacamp-exercises.html)\n",
      "- [SCT Design (R)](https://github.com/datacamp/testwhat/wiki)\n",
      "- [SCT Design (Python)](https://github.com/datacamp/pythonwhat/wiki)\n",
      "- [Style Guide](/teach/style-guide.html) -->\n",
      "\n",
      "\n",
      "      </div>\n",
      "    </div>\n",
      "\n",
      "    \n",
      "\n",
      "  </body>\n",
      "\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import package\n",
    "import requests\n",
    "\n",
    "# Specify the url: url\n",
    "url = \"http://docs.datacamp.com/teach/\"\n",
    "\n",
    "# Packages the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extract the response: text\n",
    "text = r.text\n",
    "\n",
    "# Print the html\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing HTML with BeautifulSoup\n",
    "In this interactive exercise, you'll learn how to use the BeautifulSoup package to parse, prettify and extract information from HTML. You'll scrape the data from the webpage of Guido van Rossum, Python's very own [Benevolent Dictator for Life](https://en.wikipedia.org/wiki/Benevolent_dictator_for_life). In the following exercises, you'll prettify the HTML and then extract the text and the hyperlinks.\n",
    "\n",
    "The URL of interest is `url = 'https://www.python.org/~guido/'`.\n",
    "\n",
    "**Instructions**\n",
    "* Import the function `BeautifulSoup` from the package `bs4`.\n",
    "* Assign the URL of interest to the variable `url`.\n",
    "* Package the request to the URL, send the request and catch the response with a single function `requests.get()`, assigning the response to the variable `r`.\n",
    "* Use the text attribute of the object `r` to return the HTML of the webpage as a string; store the result in a variable `html_doc`.\n",
    "* Create a BeautifulSoup object soup from the resulting HTML using the function `BeautifulSoup()`.\n",
    "* Use the method `prettify()` on soup and assign the result to `pretty_soup`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   Guido's Personal Home Page\n",
      "  </title>\n",
      " </head>\n",
      " <body bgcolor=\"#FFFFFF\" text=\"#000000\">\n",
      "  <h1>\n",
      "   <a href=\"pics.html\">\n",
      "    <img border=\"0\" src=\"images/IMG_2192.jpg\"/>\n",
      "   </a>\n",
      "   Guido van Rossum - Personal Home Page\n",
      "  </h1>\n",
      "  <p>\n",
      "   <a href=\"http://www.washingtonpost.com/wp-srv/business/longterm/microsoft/stories/1998/raymond120398.htm\">\n",
      "    <i>\n",
      "     \"Gawky and proud of it.\"\n",
      "    </i>\n",
      "   </a>\n",
      "  </p>\n",
      "  <h3>\n",
      "   <a href=\"http://metalab.unc.edu/Dave/Dr-Fun/df200004/df20000406.jpg\">\n",
      "    Who\n",
      "I Am\n",
      "   </a>\n",
      "  </h3>\n",
      "  <p>\n",
      "   Read\n",
      "my\n",
      "   <a href=\"http://neopythonic.blogspot.com/2016/04/kings-day-speech.html\">\n",
      "    \"King's\n",
      "Day Speech\"\n",
      "   </a>\n",
      "   for some inspiration.\n",
      "  </p>\n",
      "  <p>\n",
      "   I am the author of the\n",
      "   <a href=\"http://www.python.org\">\n",
      "    Python\n",
      "   </a>\n",
      "   programming language.  See also my\n",
      "   <a href=\"Resume.html\">\n",
      "    resume\n",
      "   </a>\n",
      "   and my\n",
      "   <a href=\"Publications.html\">\n",
      "    publications list\n",
      "   </a>\n",
      "   , a\n",
      "   <a href=\"bio.html\">\n",
      "    brief bio\n",
      "   </a>\n",
      "   , assorted\n",
      "   <a href=\"http://legacy.python.org/doc/essays/\">\n",
      "    writings\n",
      "   </a>\n",
      "   ,\n",
      "   <a href=\"http://legacy.python.org/doc/essays/ppt/\">\n",
      "    presentations\n",
      "   </a>\n",
      "   and\n",
      "   <a href=\"interviews.html\">\n",
      "    interviews\n",
      "   </a>\n",
      "   (all about Python), some\n",
      "   <a href=\"pics.html\">\n",
      "    pictures of me\n",
      "   </a>\n",
      "   ,\n",
      "   <a href=\"http://neopythonic.blogspot.com\">\n",
      "    my new blog\n",
      "   </a>\n",
      "   , and\n",
      "my\n",
      "   <a href=\"http://www.artima.com/weblogs/index.jsp?blogger=12088\">\n",
      "    old\n",
      "blog\n",
      "   </a>\n",
      "   on Artima.com.  I am\n",
      "   <a href=\"https://twitter.com/gvanrossum\">\n",
      "    @gvanrossum\n",
      "   </a>\n",
      "   on Twitter.  I\n",
      "also have\n",
      "a\n",
      "   <a href=\"https://plus.google.com/u/0/115212051037621986145/posts\">\n",
      "    G+\n",
      "profile\n",
      "   </a>\n",
      "   .\n",
      "  </p>\n",
      "  <p>\n",
      "   In January 2013 I joined\n",
      "   <a href=\"http://www.dropbox.com\">\n",
      "    Dropbox\n",
      "   </a>\n",
      "   .  I work on various Dropbox\n",
      "products and have 50% for my Python work, no strings attached.\n",
      "Previously, I have worked for Google, Elemental Security, Zope\n",
      "Corporation, BeOpen.com, CNRI, CWI, and SARA.  (See\n",
      "my\n",
      "   <a href=\"Resume.html\">\n",
      "    resume\n",
      "   </a>\n",
      "   .)  I created Python while at CWI.\n",
      "  </p>\n",
      "  <h3>\n",
      "   How to Reach Me\n",
      "  </h3>\n",
      "  <p>\n",
      "   You can send email for me to guido (at) python.org.\n",
      "I read everything sent there, but if you ask\n",
      "me a question about using Python, it's likely that I won't have time\n",
      "to answer it, and will instead refer you to\n",
      "help (at) python.org,\n",
      "   <a href=\"http://groups.google.com/groups?q=comp.lang.python\">\n",
      "    comp.lang.python\n",
      "   </a>\n",
      "   or\n",
      "   <a href=\"http://stackoverflow.com\">\n",
      "    StackOverflow\n",
      "   </a>\n",
      "   .  If you need to\n",
      "talk to me on the phone or send me something by snail mail, send me an\n",
      "email and I'll gladly email you instructions on how to reach me.\n",
      "  </p>\n",
      "  <h3>\n",
      "   My Name\n",
      "  </h3>\n",
      "  <p>\n",
      "   My name often poses difficulties for Americans.\n",
      "  </p>\n",
      "  <p>\n",
      "   <b>\n",
      "    Pronunciation:\n",
      "   </b>\n",
      "   in Dutch, the \"G\" in Guido is a hard G,\n",
      "pronounced roughly like the \"ch\" in Scottish \"loch\".  (Listen to the\n",
      "   <a href=\"guido.au\">\n",
      "    sound clip\n",
      "   </a>\n",
      "   .)  However, if you're\n",
      "American, you may also pronounce it as the Italian \"Guido\".  I'm not\n",
      "too worried about the associations with mob assassins that some people\n",
      "have. :-)\n",
      "  </p>\n",
      "  <p>\n",
      "   <b>\n",
      "    Spelling:\n",
      "   </b>\n",
      "   my last name is two words, and I'd like to keep it\n",
      "that way, the spelling on some of my credit cards notwithstanding.\n",
      "Dutch spelling rules dictate that when used in combination with my\n",
      "first name, \"van\" is not capitalized: \"Guido van Rossum\".  But when my\n",
      "last name is used alone to refer to me, it is capitalized, for\n",
      "example: \"As usual, Van Rossum was right.\"\n",
      "  </p>\n",
      "  <p>\n",
      "   <b>\n",
      "    Alphabetization:\n",
      "   </b>\n",
      "   in America, I show up in the alphabet under\n",
      "\"V\".  But in Europe, I show up under \"R\".  And some of my friends put\n",
      "me under \"G\" in their address book...\n",
      "  </p>\n",
      "  <h3>\n",
      "   More Hyperlinks\n",
      "  </h3>\n",
      "  <ul>\n",
      "   <li>\n",
      "    Here's a collection of\n",
      "    <a href=\"http://legacy.python.org/doc/essays/\">\n",
      "     essays\n",
      "    </a>\n",
      "    relating to Python\n",
      "that I've written, including the foreword I wrote for Mark Lutz' book\n",
      "\"Programming Python\".\n",
      "    <p>\n",
      "    </p>\n",
      "   </li>\n",
      "   <li>\n",
      "    I own the official\n",
      "    <a href=\"images/license.jpg\">\n",
      "     <img align=\"center\" border=\"0\" height=\"75\" src=\"images/license_thumb.jpg\" width=\"100\"/>\n",
      "     Python license.\n",
      "    </a>\n",
      "    <p>\n",
      "    </p>\n",
      "   </li>\n",
      "  </ul>\n",
      "  <h3>\n",
      "   The Audio File Formats FAQ\n",
      "  </h3>\n",
      "  <p>\n",
      "   I was the original creator and maintainer of the Audio File Formats\n",
      "FAQ.  It is now maintained by Chris Bagwell\n",
      "at\n",
      "   <a href=\"http://www.cnpbagwell.com/audio-faq\">\n",
      "    http://www.cnpbagwell.com/audio-faq\n",
      "   </a>\n",
      "   .  And here is a link to\n",
      "   <a href=\"http://sox.sourceforge.net/\">\n",
      "    SOX\n",
      "   </a>\n",
      "   , to which I contributed\n",
      "some early code.\n",
      "  </p>\n",
      "  <hr/>\n",
      "  <a href=\"images/internetdog.gif\">\n",
      "   \"On the Internet, nobody knows you're\n",
      "a dog.\"\n",
      "  </a>\n",
      "  <hr/>\n",
      " </body>\n",
      "</html>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\stat_tools\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\stat_tools\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify url: url\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extracts the response as html: html_doc\n",
    "html_doc = r.text\n",
    "\n",
    "# Create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html_doc)\n",
    "\n",
    "# Prettify the BeautifulSoup object: pretty_soup\n",
    "pretty_soup = soup.prettify()\n",
    "\n",
    "# Print the response\n",
    "print(pretty_soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning a webpage into data using BeautifulSoup: getting the text\n",
    "As promised, in the following exercises, you'll learn the basics of extracting information from HTML soup. In this exercise, you'll figure out how to extract the text from the BDFL's webpage, along with printing the webpage's title.\n",
    "\n",
    "**Instruction**\n",
    "* In the sample code, the HTML response object `html_doc` has already been created: your first task is to Soupify it using the function `BeatifulSoup()` and to assign the resulting soup to the variable `soup`.\n",
    "* Extract the title from the HTML soup soup using the attribute title and assign the result to `guido_title`.\n",
    "* Print the title of Guido's webpage to the shell using the `print()` function.\n",
    "* Extract the text from the HTML soup `soup` using the method `get_text()` and assign to `guido_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Guido's Personal Home Page</title>\n",
      "\n",
      "\n",
      "Guido's Personal Home Page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Guido van Rossum - Personal Home Page\n",
      "\"Gawky and proud of it.\"\n",
      "Who\n",
      "I Am\n",
      "Read\n",
      "my \"King's\n",
      "Day Speech\" for some inspiration.\n",
      "\n",
      "I am the author of the Python\n",
      "programming language.  See also my resume\n",
      "and my publications list, a brief bio, assorted writings, presentations and interviews (all about Python), some\n",
      "pictures of me,\n",
      "my new blog, and\n",
      "my old\n",
      "blog on Artima.com.  I am\n",
      "@gvanrossum on Twitter.  I\n",
      "also have\n",
      "a G+\n",
      "profile.\n",
      "\n",
      "In January 2013 I joined\n",
      "Dropbox.  I work on various Dropbox\n",
      "products and have 50% for my Python work, no strings attached.\n",
      "Previously, I have worked for Google, Elemental Security, Zope\n",
      "Corporation, BeOpen.com, CNRI, CWI, and SARA.  (See\n",
      "my resume.)  I created Python while at CWI.\n",
      "\n",
      "How to Reach Me\n",
      "You can send email for me to guido (at) python.org.\n",
      "I read everything sent there, but if you ask\n",
      "me a question about using Python, it's likely that I won't have time\n",
      "to answer it, and will instead refer you to\n",
      "help (at) python.org,\n",
      "comp.lang.python or\n",
      "StackOverflow.  If you need to\n",
      "talk to me on the phone or send me something by snail mail, send me an\n",
      "email and I'll gladly email you instructions on how to reach me.\n",
      "\n",
      "My Name\n",
      "My name often poses difficulties for Americans.\n",
      "\n",
      "Pronunciation: in Dutch, the \"G\" in Guido is a hard G,\n",
      "pronounced roughly like the \"ch\" in Scottish \"loch\".  (Listen to the\n",
      "sound clip.)  However, if you're\n",
      "American, you may also pronounce it as the Italian \"Guido\".  I'm not\n",
      "too worried about the associations with mob assassins that some people\n",
      "have. :-)\n",
      "\n",
      "Spelling: my last name is two words, and I'd like to keep it\n",
      "that way, the spelling on some of my credit cards notwithstanding.\n",
      "Dutch spelling rules dictate that when used in combination with my\n",
      "first name, \"van\" is not capitalized: \"Guido van Rossum\".  But when my\n",
      "last name is used alone to refer to me, it is capitalized, for\n",
      "example: \"As usual, Van Rossum was right.\"\n",
      "\n",
      "Alphabetization: in America, I show up in the alphabet under\n",
      "\"V\".  But in Europe, I show up under \"R\".  And some of my friends put\n",
      "me under \"G\" in their address book...\n",
      "\n",
      "\n",
      "More Hyperlinks\n",
      "\n",
      "Here's a collection of essays relating to Python\n",
      "that I've written, including the foreword I wrote for Mark Lutz' book\n",
      "\"Programming Python\".\n",
      "I own the official \n",
      "Python license.\n",
      "\n",
      "The Audio File Formats FAQ\n",
      "I was the original creator and maintainer of the Audio File Formats\n",
      "FAQ.  It is now maintained by Chris Bagwell\n",
      "at http://www.cnpbagwell.com/audio-faq.  And here is a link to\n",
      "SOX, to which I contributed\n",
      "some early code.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\"On the Internet, nobody knows you're\n",
      "a dog.\"\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\stat_tools\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\stat_tools\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify url: url\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extract the response as html: html_doc\n",
    "html_doc = r.text\n",
    "\n",
    "# Create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html_doc)\n",
    "\n",
    "# Get the title of Guido's webpage: guido_title\n",
    "guido_title = soup.title\n",
    "\n",
    "# Print the title of Guido's webpage to the shell\n",
    "print(guido_title)\n",
    "\n",
    "# Get Guido's text: guido_text\n",
    "guido_text = soup.text\n",
    "\n",
    "# Print Guido's text to the shell\n",
    "print(guido_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning a webpage into data using BeautifulSoup: getting the hyperlinks\n",
    "In this exercise, you'll figure out how to extract the URLs of the hyperlinks from the BDFL's webpage. In the process, you'll become close friends with the soup method `find_all()`.\n",
    "\n",
    "**Instructions**\n",
    "* Use the method `find_all()` to find all hyperlinks in `soup`, remembering that hyperlinks are defined by the HTML tag `<a>`; store the result in the variable `a_tags`.\n",
    "* The variable `a_tags` is a results set: your job now is to enumerate over it, using a for loop and to print the actual URLs of the hyperlinks; to do this, for every element link in `a_tags`, you want to `print()` `link.get('href')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Guido's Personal Home Page</title>\n",
      "pics.html\n",
      "http://www.washingtonpost.com/wp-srv/business/longterm/microsoft/stories/1998/raymond120398.htm\n",
      "http://metalab.unc.edu/Dave/Dr-Fun/df200004/df20000406.jpg\n",
      "http://neopythonic.blogspot.com/2016/04/kings-day-speech.html\n",
      "http://www.python.org\n",
      "Resume.html\n",
      "Publications.html\n",
      "bio.html\n",
      "http://legacy.python.org/doc/essays/\n",
      "http://legacy.python.org/doc/essays/ppt/\n",
      "interviews.html\n",
      "pics.html\n",
      "http://neopythonic.blogspot.com\n",
      "http://www.artima.com/weblogs/index.jsp?blogger=12088\n",
      "https://twitter.com/gvanrossum\n",
      "https://plus.google.com/u/0/115212051037621986145/posts\n",
      "http://www.dropbox.com\n",
      "Resume.html\n",
      "http://groups.google.com/groups?q=comp.lang.python\n",
      "http://stackoverflow.com\n",
      "guido.au\n",
      "http://legacy.python.org/doc/essays/\n",
      "images/license.jpg\n",
      "http://www.cnpbagwell.com/audio-faq\n",
      "http://sox.sourceforge.net/\n",
      "images/internetdog.gif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\stat_tools\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\stat_tools\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify url\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extracts the response as html: html_doc\n",
    "html_doc = r.text\n",
    "\n",
    "# create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html_doc)\n",
    "\n",
    "# Print the title of Guido's webpage\n",
    "print(soup.title)\n",
    "\n",
    "# Find all 'a' tags (which define hyperlinks): a_tags\n",
    "a_tags = soup.find_all('a')\n",
    "\n",
    "# Print the URLs to the shell\n",
    "for link in a_tags:\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with APIs to Import Data from the Web\n",
    "## Pop quiz: What exactly is a JSON?\n",
    "**Possible Answers**\n",
    "1. JSONs consist of key-value pairs.\n",
    "2. JSONs are human-readable.\n",
    "3. The JSON file format arose out of a growing need for real-time server-to-browser communication.\n",
    "4. The function json.load() will load the JSON into Python as a list.\n",
    "5. The function json.load() will load the JSON into Python as a dictionary.\n",
    "*Answer: 4*\n",
    "\n",
    "## Loading and exploring a JSON\n",
    "Now that you know what a JSON is, you'll load one into your Python environment and explore it yourself. Here, you'll load the JSON `'a_movie.json'` into the variable `json_data`, which will be a dictionary. You'll then explore the JSON contents by printing the key-value pairs of `json_data` to the shell.\n",
    "\n",
    "**Instructions**\n",
    "* Load the JSON `'a_movie.json'` into the variable `json_data` within the context provided by the with statement. To do so, use the function `json.load()` within the context manager.\n",
    "* Use a `for` loop to print all key-value pairs in the dictionary `json_data`. Recall that you can access a value in a dictionary using the syntax: dictionary`[`key`]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'cp932' codec can't decode byte 0xef in position 0: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-feabd70f5ca9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Load JSON: json_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"a_movie.json\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mjson_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Print each key-value pair in json_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\stat_tools\\Anaconda3\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m     \"\"\"\n\u001b[1;32m--> 296\u001b[1;33m     return loads(fp.read(),\n\u001b[0m\u001b[0;32m    297\u001b[0m         \u001b[0mcls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'cp932' codec can't decode byte 0xef in position 0: illegal multibyte sequence"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# Load JSON: json_data\n",
    "with open(\"a_movie.json\") as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "\n",
    "# Print each key-value pair in json_data\n",
    "for k in json_data.keys():\n",
    "    print(k + ': ', json_data[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pop quiz: Exploring your JSON\n",
    "Load the JSON `'a_movie.json'` into a variable, which will be a dictionary. Do so by copying, pasting and executing the following code in the IPython Shell:\n",
    "```\n",
    "import json\n",
    "with open(\"a_movie.json\") as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "```\n",
    "Print the values corresponding to the keys `'Title'` and `'Year'` and answer the following question about the movie that the JSON describes:\n",
    "\n",
    "Which of the following statements is true of the movie in question?\n",
    "**Possible Answers**\n",
    "1. The title is `'Kung Fu Panda'` and the year is `2010`.\n",
    "2. The title is `'Kung Fu Panda'` and the year is `2008`.\n",
    "3. The title is `'The Social Network'` and the year is `2010`.\n",
    "4. The title is `'The Social Network'` and the year is `2008`.\n",
    "\n",
    "*Answer: 3*\n",
    "\n",
    "## Pop quiz: What's an API?\n",
    "Which of the following statements about APIs is NOT true?\n",
    "\n",
    "**Possible Answers**\n",
    "1. An API is a set of protocols and routines for building and interacting with software applications.\n",
    "2. API is an acronym and is short for Application Program interface.\n",
    "3. It is common to pull data from APIs in the JSON file format.\n",
    "4. All APIs transmit data only in the JSON file format.\n",
    "5. An API is a bunch of code that allows two software programs to communicate with each other.\n",
    "\n",
    "*Answer: 4*\n",
    "\n",
    "## API requests\n",
    "Now it's your turn to pull some movie data down from the Open Movie Database (OMDB) using their API. The movie you'll query the API about is The Social Network. Recall that, in the video, to query the API about the movie Hackers, Hugo's query string was `'http://www.omdbapi.com/?t=hackers`' and had a single argument `t=hackers`.\n",
    "\n",
    "Note: recently, OMDB has changed their API: you now also have to specify an API key. This means you'll have to add another argument to the URL: `apikey=ff21610b`.\n",
    "\n",
    "**Instructions**\n",
    "* Import the `requests` package.\n",
    "* Assign to the variable `url` the URL of interest in order to query `'http://www.omdbapi.com'` for the data corresponding to the movie The Social Network. The query string should have two arguments: `apikey=ff21610b` and `t=social+network`. You can combine them as follows: `apikey=ff21610b&t=social+network`.\n",
    "* Print the text of the reponse object r by using its text attribute and passing the result to the `print()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Title\":\"The Social Network\",\"Year\":\"2010\",\"Rated\":\"PG-13\",\"Released\":\"01 Oct 2010\",\"Runtime\":\"120 min\",\"Genre\":\"Biography, Drama\",\"Director\":\"David Fincher\",\"Writer\":\"Aaron Sorkin (screenplay), Ben Mezrich (book)\",\"Actors\":\"Jesse Eisenberg, Rooney Mara, Bryan Barter, Dustin Fitzsimons\",\"Plot\":\"Harvard student Mark Zuckerberg creates the social networking site that would become known as Facebook, but is later sued by two brothers who claimed he stole their idea, and the co-founder who was later squeezed out of the business.\",\"Language\":\"English, French\",\"Country\":\"USA\",\"Awards\":\"Won 3 Oscars. Another 165 wins & 169 nominations.\",\"Poster\":\"https://images-na.ssl-images-amazon.com/images/M/MV5BMTM2ODk0NDAwMF5BMl5BanBnXkFtZTcwNTM1MDc2Mw@@._V1_SX300.jpg\",\"Ratings\":[{\"Source\":\"Internet Movie Database\",\"Value\":\"7.7/10\"},{\"Source\":\"Rotten Tomatoes\",\"Value\":\"96%\"},{\"Source\":\"Metacritic\",\"Value\":\"95/100\"}],\"Metascore\":\"95\",\"imdbRating\":\"7.7\",\"imdbVotes\":\"514,092\",\"imdbID\":\"tt1285016\",\"Type\":\"movie\",\"DVD\":\"11 Jan 2011\",\"BoxOffice\":\"$96,400,000\",\"Production\":\"Columbia Pictures\",\"Website\":\"http://www.thesocialnetwork-movie.com/\",\"Response\":\"True\"}\n"
     ]
    }
   ],
   "source": [
    "# Import requests package\n",
    "import requests\n",
    "\n",
    "# Assign URL to variable: url\n",
    "url = 'http://www.omdbapi.com/?apikey=ff21610b&t=social+network'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Print the text of the response\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSONfrom the web to Python\n",
    "Wow, congrats! You've just queried your first API programmatically in Python and printed the text of the response to the shell. However, as you know, your response is actually a JSON, so you can do one step better and decode the JSON. You can then print the key-value pairs of the resulting dictionary. That's what you're going to do now!\n",
    "\n",
    "**Instructions**\n",
    "* Pass the variable url to the `requests.get()` function in order to send the relevant request and catch the response, assigning the resultant response message to the variable `r`.\n",
    "* Apply the `json()` method to the response object `r` and store the resulting dictionary in the variable `json_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:  The Social Network\n",
      "Year:  2010\n",
      "Rated:  PG-13\n",
      "Released:  01 Oct 2010\n",
      "Runtime:  120 min\n",
      "Genre:  Biography, Drama\n",
      "Director:  David Fincher\n",
      "Writer:  Aaron Sorkin (screenplay), Ben Mezrich (book)\n",
      "Actors:  Jesse Eisenberg, Rooney Mara, Bryan Barter, Dustin Fitzsimons\n",
      "Plot:  Harvard student Mark Zuckerberg creates the social networking site that would become known as Facebook, but is later sued by two brothers who claimed he stole their idea, and the co-founder who was later squeezed out of the business.\n",
      "Language:  English, French\n",
      "Country:  USA\n",
      "Awards:  Won 3 Oscars. Another 165 wins & 169 nominations.\n",
      "Poster:  https://images-na.ssl-images-amazon.com/images/M/MV5BMTM2ODk0NDAwMF5BMl5BanBnXkFtZTcwNTM1MDc2Mw@@._V1_SX300.jpg\n",
      "Ratings:  [{'Source': 'Internet Movie Database', 'Value': '7.7/10'}, {'Source': 'Rotten Tomatoes', 'Value': '96%'}, {'Source': 'Metacritic', 'Value': '95/100'}]\n",
      "Metascore:  95\n",
      "imdbRating:  7.7\n",
      "imdbVotes:  514,092\n",
      "imdbID:  tt1285016\n",
      "Type:  movie\n",
      "DVD:  11 Jan 2011\n",
      "BoxOffice:  $96,400,000\n",
      "Production:  Columbia Pictures\n",
      "Website:  http://www.thesocialnetwork-movie.com/\n",
      "Response:  True\n"
     ]
    }
   ],
   "source": [
    "# Import package\n",
    "import requests\n",
    "\n",
    "# Assign URL to variable: url\n",
    "url = 'http://www.omdbapi.com/?apikey=ff21610b&t=social+network'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Decode the JSON data into a dictionary: json_data\n",
    "json_data = r.json()\n",
    "\n",
    "# Print each key-value pair in json_data\n",
    "for k in json_data.keys():\n",
    "    print(k + ': ', json_data[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking out the Wikipedia API\n",
    "You're doing so well and having so much fun that we're going to throw one more API at you: the Wikipedia API (documented [here](https://www.mediawiki.org/wiki/API:Main_page)). You'll figure out how to find and extract information from the Wikipedia page for Pizza. What gets a bit wild here is that your query will return nested JSONs, that is, JSONs with JSONs, but Python can handle that because it will translate them into dictionaries within dictionaries.\n",
    "\n",
    "The URL that requests the relevant query from the Wikipedia API is\n",
    "```\n",
    "https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&exintro=&titles=pizza\n",
    "```\n",
    "**Instructions**\n",
    "* Assign the relevant URL to the variable `url`.\n",
    "* Apply the `json()` method to the response object `r` and store the resulting dictionary in the variable `json_data`.\n",
    "* The variable `pizza_extract` holds the HTML of an extract from Wikipedia's Pizza page as a string; use the function `print()` to print this string to the shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p><b>Pizza</b> is a yeasted flatbread typically topped with tomato sauce and cheese and baked in an oven. It is commonly topped with a selection of meats, vegetables and condiments.</p>\n",
      "<p>The term <i>pizza</i> was first recorded in the 10th century, in a Latin manuscript from Gaeta in Central Italy. Modern pizza was invented in Naples, Italy, and the dish and its variants have since become popular and common in many areas of the world. In 2009, upon Italy's request, Neapolitan pizza was safeguarded in the European Union as a Traditional Speciality Guaranteed dish. <i>Associazione Verace Pizza Napoletana</i> (True Neapolitan Pizza Association), a non-profit organization founded in 1984 with headquarters in Naples, aims to \"promote and protect... the true Neapolitan pizza\".</p>\n",
      "<p>Pizza is sold fresh or frozen, either whole or in portions, and is a common fast food item in Europe and North America. Various types of ovens are used to cook them and many varieties exist. Several similar dishes are prepared from ingredients commonly used in pizza preparation, such as calzone and stromboli.</p>\n",
      "<p></p>\n",
      "\n",
      "<p></p>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import package\n",
    "import requests\n",
    "\n",
    "# Assign URL to variable: url\n",
    "url = 'https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&exintro=&titles=pizza'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Decode the JSON data into a dictionary: json_data\n",
    "json_data = r.json()\n",
    "\n",
    "# Print the Wikipedia page extract\n",
    "pizza_extract = json_data['query']['pages']['24768']['extract']\n",
    "print(pizza_extract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diving deep into the Twitter API\n",
    "## API Authentication\n",
    "The package `tweepy` is great at handling all the Twitter API OAuth Authentication details for you. All you need to do is pass it your authentication credentials. In this interactive exercise, we have created some mock authentication credentials (if you wanted to replicate this at home, you would need to create a [Twitter App](https://apps.twitter.com/) as Hugo detailed in the video). Your task is to pass these credentials to tweepy's OAuth handler.\n",
    "\n",
    "**Instruction**\n",
    "* Import the package `tweepy`.\n",
    "* Pass the parameters `consumer_key` and `consumer_secret` to the function `tweepy.OAuthHandler()`.\n",
    "* Complete the passing of OAuth credentials to the OAuth handler `auth` by applying to it the method `set_access_token()`, along with arguments access_token and access_token_secret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package\n",
    "# !pip install tweepy\n",
    "import tweepy\n",
    "\n",
    "# Store OAuth authentication credentials in relevant variables\n",
    "access_token = \"1092294848-aHN7DcRP9B4VMTQIhwqOYiB14YkW92fFO8k8EPy\"\n",
    "access_token_secret = \"X4dHmhPfaksHcQ7SCbmZa2oYBBVSD2g8uIHXsp5CTaksx\"\n",
    "consumer_key = \"nZ6EA0FxZ293SxGNg8g8aP0HM\"\n",
    "consumer_secret = \"fJGEodwe3KiKUnsYJC3VRndj7jevVvXbK2D5EiJ2nehafRgA6i\"\n",
    "\n",
    "# Pass OAuth details to tweepy's OAuth handler\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming tweets\n",
    "Now that you have set up your authentication credentials, it is time to stream some tweets! We have already defined the tweet stream listener class, `MyStreamListener`, just as Hugo did in the introductory video. You can find the code for the tweet stream listener class [here](https://gist.github.com/hugobowne/18f1c0c0709ed1a52dc5bcd462ac69f4).\n",
    "\n",
    "Your task is to create the `Stream` object and to filter tweets according to particular keywords.\n",
    "\n",
    "**Instructions**\n",
    "* Create your `Stream` object with authentication by passing `tweepy.Stream()` the authentication handler `auth` and the Stream listener `l`;\n",
    "* To filter Twitter streams, pass to the track argument in `stream.filter()` a list containing the desired keywords `'clinton'`, `'trump'`, `'sanders'`, and `'cruz'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Stream listener\n",
    "l = MyStreamListener()\n",
    "\n",
    "# Create you Stream object with authentication\n",
    "stream = tweepy.Stream(auth, l)\n",
    "\n",
    "# Filter Twitter Streams to capture data by the keywords:\n",
    "stream.filter(track = ['clinton', 'trump', 'sanders', 'cruz'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and explore your Twitter data\n",
    "Now that you've got your Twitter data sitting locally in a text file, it's time to explore it! This is what you'll do in the next few interactive exercises. In this exercise, you'll read the Twitter data into a list: `tweets_data`.\n",
    "\n",
    "**Instructions**\n",
    "* Assign the filename `'tweets.txt'` to the variable `tweets_data_path`.\n",
    "* Initialize `tweets_data` as an empty list to store the tweets in.\n",
    "* Within the for loop initiated by for line in `tweets_file`:, load each tweet into a variable, `tweet`, using `json.loads()`, then append tweet to `tweets_data` using the `append()` method.\n",
    "* Hit submit and check out the keys of the first tweet dictionary printed to the shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package\n",
    "import json\n",
    "\n",
    "# String of path to file: tweets_data_path\n",
    "tweets_data_path = 'tweets.txt'\n",
    "\n",
    "# Initialize empty list to store tweets: tweets_data\n",
    "tweets_data = []\n",
    "\n",
    "# Open connection to file\n",
    "tweets_file = open(tweets_data_path, \"r\")\n",
    "\n",
    "# Read in tweets and store in list: tweets_data\n",
    "for line in tweets_file:\n",
    "    tweet = json.loads(line)\n",
    "    tweets_data.append(tweet)\n",
    "\n",
    "# Close connection to file\n",
    "tweets_file.close()\n",
    "\n",
    "# Print the keys of the first tweet dict\n",
    "print(tweets_data[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter data to DataFrame\n",
    "Now you have the Twitter data in a list of dictionaries, `tweets_data`, where each dictionary corresponds to a single tweet. Next, you're going to extract the text and language of each tweet. The text in a tweet, `t1`, is stored as the value `t1['text']`; similarly, the language is stored in `t1['lang']`. Your task is to build a DataFrame in which each row is a tweet and the columns are `'text'` and `'lang'`\n",
    "\n",
    "**Instructions**\n",
    "* Use `pd.DataFrame()` to construct a DataFrame of tweet texts and languages; to do so, the first argument should be `tweets_data`, a list of dictionaries. The second argument to `pd.DataFrame()` is a list of the keys you wish to have as columns. Assign the result of the `pd.DataFrame()` call to df.\n",
    "* Print the head of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import package\n",
    "import pandas as pd\n",
    "\n",
    "# Build DataFrame of tweet texts and languages\n",
    "df = pd.DataFrame(tweets_data, columns=['text', 'lang'])\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A little bit of Twitter text analysis\n",
    "Now that you have your DataFrame of tweets set up, you're going to do a bit of text analysis to count how many tweets contain the words `'clinton'`, `'trump'`, `'sanders'` and `'cruz'`. In the pre-exercise code, we have defined the following function `word_in_text()`, which will tell you whether the first argument (a word) occurs within the 2nd argument (a tweet).\n",
    "\n",
    "```\n",
    "import re\n",
    "\n",
    "def word_in_text(word, tweet):\n",
    "    word = word.lower()\n",
    "    text = tweet.lower()\n",
    "    match = re.search(word, tweet)\n",
    "\n",
    "    if match:\n",
    "        return True\n",
    "    return False\n",
    "```\n",
    "\n",
    "You're going to iterate over the rows of the DataFrame and calculate how many tweets contain each of our keywords! The list of objects for each candidate has been initialized to 0.\n",
    "\n",
    "**Instructions**\n",
    "* Within the `for` loop `for index, row in df.iterrows()`:, the code currently increases the value of `clinton` by `1` each time a tweet mentioning 'Clinton' is encountered; complete the code so that the same happens for `trump`, `sanders` and `cruz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize list to store tweet counts\n",
    "[clinton, trump, sanders, cruz] = [0, 0, 0, 0]\n",
    "\n",
    "# Iterate through df, counting the number of tweets in which\n",
    "# each candidate is mentioned\n",
    "for index, row in df.iterrows():\n",
    "    clinton += word_in_text('clinton', row['text'])\n",
    "    trump += word_in_text('trump', row['text'])\n",
    "    sanders += word_in_text('sanders', row['text'])\n",
    "    cruz += word_in_text('cruz', row['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting your Twitter data\n",
    "Now that you have the number of tweets that each candidate was mentioned in, you can plot a bar chart of this data. You'll use the statistical data visualization library `seaborn`, which you may not have seen before, but we'll guide you through. You'll first import `seaborn` as `sns`. You'll then construct a barplot of the data using `sns.barplot`, passing it two arguments:\n",
    "\n",
    "1. a list of labels and\n",
    "2. a list containing the variables you wish to plot (`clinton`, `trump` and so on.)\n",
    "\n",
    "Hopefully, you'll see that Trump was unreasonably represented! We have already run the previous exercise solutions in your environment.\n",
    "\n",
    "**Instructions**\n",
    "* Import both `matplotlib.pyplot` and `seaborn` using the aliases `plt` and `sns`, respectively.\n",
    "* Complete the arguments of `sns.barplot`: the first argument should be the labels to appear on the x-axis; the second argument should be the list of the variables you wish to plot, as produced in the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "# Create a list of labels:cd\n",
    "cd = ['clinton', 'trump', 'sanders', 'cruz']\n",
    "\n",
    "# Plot histogram\n",
    "ax = sns.barplot(cd, [clinton, trump, sanders, cruz])\n",
    "ax.set(ylabel=\"count\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
